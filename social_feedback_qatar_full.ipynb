{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09fe9752",
   "metadata": {},
   "source": [
    "# Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Reddit ÙˆFacebook ÙˆInstagram Ø­ÙˆÙ„ Ø§Ù„Ø¬Ù‡Ø§Øª Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ© ÙÙŠ Ù‚Ø·Ø±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881ea39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\ahd\\anaconda3\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: googletrans==4.0.0-rc1 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: langdetect in c:\\users\\ahd\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: pandas in c:\\users\\ahd\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\ahd\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: google-search-results in c:\\users\\ahd\\anaconda3\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from praw) (0.58.0)\n",
      "Requirement already satisfied: six in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from google-search-results) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahd\\anaconda3\\lib\\site-packages (from requests->google-search-results) (1.26.16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install praw googletrans==4.0.0-rc1 langdetect pandas openpyxl google-search-results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5f6e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Reddit data saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "from datetime import datetime\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id='client_id',\n",
    "    client_secret='client_secret,\n",
    "    user_agent='user_agent'\n",
    ")\n",
    "\n",
    "keywords = {\n",
    "    \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ…\": [\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ…\", \"Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙÙŠ Ù‚Ø·Ø±\", \"ministry of education qatar\"],\n",
    "    \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø©\": [\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø©\", \"Ø§Ù„ØµØ­Ø© ÙÙŠ Ù‚Ø·Ø±\", \"MOPH Qatar\"],\n",
    "    \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¬Ø§Ø±Ø© ÙˆØ§Ù„ØµÙ†Ø§Ø¹Ø©\": [\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¬Ø§Ø±Ø©\", \"Ø§Ù„ØªØ¬Ø§Ø±Ø© Ù‚Ø·Ø±\", \"ministry of commerce qatar\"],\n",
    "    \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª\": [\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª\", \"mots qatar\"],\n",
    "    \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¨Ù„Ø¯ÙŠØ©\": [\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¨Ù„Ø¯ÙŠØ©\", \"municipality qatar\"],\n",
    "    \"Ø£Ø´ØºØ§Ù„\": [\"Ù‡ÙŠØ¦Ø© Ø§Ù„Ø£Ø´ØºØ§Ù„\", \"ashghal\", \"ashghal qatar\"]\n",
    "}\n",
    "\n",
    "data = []\n",
    "\n",
    "for entity, terms in keywords.items():\n",
    "    for term in terms:\n",
    "        for post in reddit.subreddit(\"all\").search(term, limit=50):\n",
    "            try:\n",
    "                text = post.title + \"\\n\" + post.selftext\n",
    "                lang = detect(text)\n",
    "                ar_text = text if lang == 'ar' else translator.translate(text, dest='ar').text\n",
    "                en_text = text if lang == 'en' else translator.translate(text, dest='en').text\n",
    "                data.append({\n",
    "                    \"ID\": post.id,\n",
    "                    \"Source\": \"Reddit\",\n",
    "                    \"Entity\": entity,\n",
    "                    \"Arabic Post\": ar_text,\n",
    "                    \"English\": en_text,\n",
    "                    \"Original Lang\": lang,\n",
    "                    \"Post Type\": \"Post\",\n",
    "                    \"Hashtag\": ', '.join([t for t in text.split() if t.startswith(\"#\")]),\n",
    "                    \"Date\": datetime.fromtimestamp(post.created_utc),\n",
    "                    \"Link\": f\"https://www.reddit.com{post.permalink}\"\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(\"reddit_qatar_feedback.xlsx\", index=False)\n",
    "print(\"âœ… Reddit data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71683307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Searching Google for:  ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ù‚Ø·Ø± site:facebook.com\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ù‚Ø·Ø±  site:instagram.com\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ù‚Ø·Ø±  site:twitter.com\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© Ù‚Ø·Ø± site:facebook.com\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© Ù‚Ø·Ø± site:instagram.com\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© Ù‚Ø·Ø± site:twitter.com\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¬Ø§Ø±Ø© ÙˆØ§Ù„ØµÙ†Ø§Ø¹Ø© Ù‚Ø·Ø± site:facebook.com\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¬Ø§Ø±Ø© ÙˆØ§Ù„ØµÙ†Ø§Ø¹Ø© Ù‚Ø·Ø± site:instagram.com\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¬Ø§Ø±Ø© ÙˆØ§Ù„ØµÙ†Ø§Ø¹Ø© Ù‚Ø·Ø± site:twitter.com\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª Ù‚Ø·Ø± site:facebook.com\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª Ù‚Ø·Ø± site:instagram.com\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª Ù‚Ø·Ø± site:twitter.com\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¨Ù„Ø¯ÙŠØ© Ù‚Ø·Ø± site:facebook.com\n",
      "âŒ Error for query ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¨Ù„Ø¯ÙŠØ© Ù‚Ø·Ø± site:facebook.com: The read operation timed out\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¨Ù„Ø¯ÙŠØ© Ù‚Ø·Ø± site:instagram.com\n",
      "âŒ Error for query ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¨Ù„Ø¯ÙŠØ© Ù‚Ø·Ø± site:instagram.com: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "ğŸ” Searching Google for: ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¨Ù„Ø¯ÙŠØ© Ù‚Ø·Ø± site:twitter.com\n",
      "âŒ Error for query ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¨Ù„Ø¯ÙŠØ© Ù‚Ø·Ø± site:twitter.com: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "ğŸ” Searching Google for:  Ù‡ÙŠØ¦Ø© Ø§Ù„Ø£Ø´ØºØ§Ù„ Ù‚Ø·Ø± site:facebook.com\n",
      "âŒ Error for query  Ù‡ÙŠØ¦Ø© Ø§Ù„Ø£Ø´ØºØ§Ù„ Ù‚Ø·Ø± site:facebook.com: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "ğŸ” Searching Google for: ashghal qatar site:instagram.com\n",
      "âŒ Error for query ashghal qatar site:instagram.com: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "ğŸ” Searching Google for: ashghal qatar site:twitter.com\n",
      "âŒ Error for query ashghal qatar site:twitter.com: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "âœ… Data saved to fb_ig_from_google.xlsx\n"
     ]
    }
   ],
   "source": [
    "from serpapi import GoogleSearch\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "import time\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "# Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ SerpAPI Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù‡Ù†Ø§\n",
    "SERPAPI_API_KEY = \"SERPAPI_API_KEY\"\n",
    "\n",
    "entities = {\n",
    "    \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ù‚Ø·Ø±\": [\" ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ù‚Ø·Ø± site:facebook.com\", \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ù‚Ø·Ø±  site:instagram.com\",\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ù‚Ø·Ø±  site:twitter.com\"],\n",
    "    \" ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© Ù‚Ø·Ø±\": [\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© Ù‚Ø·Ø± site:facebook.com\", \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© Ù‚Ø·Ø± site:instagram.com\",\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© Ù‚Ø·Ø± site:twitter.com\"],\n",
    "    \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¬Ø§Ø±Ø© ÙˆØ§Ù„ØµÙ†Ø§Ø¹Ø© Ù‚Ø·Ø±\": [\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¬Ø§Ø±Ø© ÙˆØ§Ù„ØµÙ†Ø§Ø¹Ø© Ù‚Ø·Ø± site:facebook.com\", \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¬Ø§Ø±Ø© ÙˆØ§Ù„ØµÙ†Ø§Ø¹Ø© Ù‚Ø·Ø± site:instagram.com\",\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ¬Ø§Ø±Ø© ÙˆØ§Ù„ØµÙ†Ø§Ø¹Ø© Ù‚Ø·Ø± site:twitter.com\"],\n",
    "    \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª Ù‚Ø·Ø±\": [\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª Ù‚Ø·Ø± site:facebook.com\", \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª Ù‚Ø·Ø± site:instagram.com\",\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª Ù‚Ø·Ø± site:twitter.com\"],\n",
    "    \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¨Ù„Ø¯ÙŠØ© Ù‚Ø·Ø±\": [\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¨Ù„Ø¯ÙŠØ© Ù‚Ø·Ø± site:facebook.com\", \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¨Ù„Ø¯ÙŠØ© Ù‚Ø·Ø± site:instagram.com\",\"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¨Ù„Ø¯ÙŠØ© Ù‚Ø·Ø± site:twitter.com\"],\n",
    "    \"Ø£Ø´ØºØ§Ù„ Ù‚Ø·Ø±\": [\" Ù‡ÙŠØ¦Ø© Ø§Ù„Ø£Ø´ØºØ§Ù„ Ù‚Ø·Ø± site:facebook.com\", \"ashghal qatar site:instagram.com\",\"ashghal qatar site:twitter.com\"]\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for entity, queries in entities.items():\n",
    "    for query in queries:\n",
    "        print(f\"ğŸ” Searching Google for: {query}\")\n",
    "        params = {\n",
    "            \"engine\": \"google\",\n",
    "            \"q\": query,\n",
    "            \"hl\": \"ar\",\n",
    "            \"num\": 50,\n",
    "            \"api_key\": SERPAPI_API_KEY,\n",
    "            \"tbs\": \"cdr:1,cd_min:1/1/2020,cd_max:12/31/2025\"\n",
    "        }\n",
    "        search = GoogleSearch(params)\n",
    "        try:\n",
    "            results_data = search.get_dict()\n",
    "            for r in results_data.get(\"organic_results\", []):\n",
    "                title = r.get(\"title\", \"\")\n",
    "                snippet = r.get(\"snippet\", \"\")\n",
    "                url = r.get(\"link\", \"\")\n",
    "                text = title + \" \" + snippet\n",
    "                lang = detect(text)\n",
    "                ar = text if lang == \"ar\" else translator.translate(text, dest='ar').text\n",
    "                en = text if lang == \"en\" else translator.translate(text, dest='en').text\n",
    "                results.append({\n",
    "                    \"ID\": url.split(\"/\")[-1][:15],\n",
    "                    \"Source\": \"Facebook/Instagram (via Google)\",\n",
    "                    \"Entity\": entity,\n",
    "                    \"Arabic Post\": ar,\n",
    "                    \"English\": en,\n",
    "                    \"Original Lang\": lang,\n",
    "                    \"Post Type\": \"Post\",\n",
    "                    \"Hashtag\": ', '.join([t for t in text.split() if t.startswith(\"#\")]),\n",
    "                    \"Date\": \"\",\n",
    "                    \"Link\": url\n",
    "                })\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error for query {query}: {e}\")\n",
    "\n",
    "# Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "df = pd.DataFrame(results)\n",
    "df.to_excel(\"fb_ig_from_google.xlsx\", index=False)\n",
    "print(\"âœ… Data saved to fb_ig_from_google.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14804a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All data saved to final Excel file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1 = pd.read_excel(\"reddit_qatar_feedback.xlsx\")\n",
    "df2 = pd.read_excel(\"fb_ig_from_google.xlsx\")\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "merged_df.to_excel(\"qatar_social_feedback_combined.xlsx\", index=False)\n",
    "print(\"âœ… All data saved to final Excel file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff83079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
